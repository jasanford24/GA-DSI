{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ![](https://ga-dash.s3.amazonaws.com/production/assets/logo-9f88ae6c9c3871690e33280fcf557f33.png) Webscraping Project 4 Lab\n",
    "\n",
    "Week 4 | Day 4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "focus": false,
    "id": "34681254-c802-462f-829d-8894d0772d08"
   },
   "source": [
    "In this project, we will practice two major skills: collecting data by scraping a website and then building a binary predictor with Logistic Regression.\n",
    "\n",
    "We are going to collect salary information on data science jobs in a variety of markets. Then using the location, title and summary of the job we will attempt to predict the salary of the job. For job posting sites, this would be extraordinarily useful. While most listings DO NOT come with salary information (as you will see in this exercise), being to able extrapolate or predict the expected salaries from other listings can help guide negotiations.\n",
    "\n",
    "Normally, we could use regression for this task; however, we will convert this problem into classification and use Logistic Regression.\n",
    "\n",
    "- Question: Why would we want this to be a classification problem?\n",
    "- Answer: While more precision may be better, there is a fair amount of natural variance in job salaries - predicting a range be may be useful.\n",
    "\n",
    "Therefore, the first part of the assignment will be focused on scraping Indeed.com (or other sites at your team's discretion). In the second part, the focus is on using listings with salary information to build a model and predict high or low salaries and what features are predictive of that result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-16T19:48:24.087343",
     "start_time": "2016-10-16T19:48:24.067492"
    },
    "collapsed": false,
    "focus": false,
    "id": "e915023e-6b0d-4982-af2a-b1e0355f4927"
   },
   "outputs": [],
   "source": [
    "URL = \"http://www.indeed.com/jobs?q=data+scientist&l=New+York&start=20\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-16T19:48:24.100146",
     "start_time": "2016-10-16T19:48:24.091112"
    },
    "collapsed": true,
    "focus": false,
    "id": "2efefc73-064a-482d-b3b5-ddf5508cb4ec"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import bs4\n",
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-16T19:48:29.796049",
     "start_time": "2016-10-16T19:48:24.104309"
    },
    "collapsed": false,
    "focus": false,
    "id": "2c6752c4-7704-4c94-8bc0-6f13d2d0d570"
   },
   "outputs": [],
   "source": [
    "## YOUR CODE HERE\n",
    "import requests\n",
    "import bs4\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium.webdriver.common.desired_capabilities import DesiredCapabilities\n",
    "from selenium import webdriver\n",
    "\n",
    "dcap = dict(DesiredCapabilities.PHANTOMJS)\n",
    "dcap[\"phantomjs.page.settings.userAgent\"] = (\n",
    "    \"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/53.0.2785.143 Safari/537.36\"\n",
    ")\n",
    "\n",
    "driver = webdriver.PhantomJS(executable_path='/Users/Jesse/anaconda/bin/phantomJS', desired_capabilities=dcap)\n",
    "driver.set_window_size(1024, 768) \n",
    "\n",
    "URL = \"http://www.indeed.com/jobs?q=data+scientist&l=New+York&start=20\"\n",
    "\n",
    "driver.get(URL)\n",
    "page_html = driver.page_source\n",
    "soup = BeautifulSoup(page_html, \"lxml\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "focus": false,
    "id": "34070e89-9521-4b45-90c8-57a6599aac68"
   },
   "source": [
    "Now, to scale up our scraping, we need to accumulate more results. We can do this by examining the URL above.\n",
    "\n",
    "- \"http://www.indeed.com/jobs?q=data+scientist+%2420%2C000&l=New+York&start=10\"\n",
    "\n",
    "There are two query parameters here we can alter to collect more results, the `l=New+York` and the `start=10`. The first controls the location of the results (so we can try a different city). The second controls where in the results to start and gives 10 results (thus, we can keep incrementing by 10 to go further in the list)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "focus": false,
    "id": "e8beed7c-3e42-40c0-810f-5f67f8f885a0"
   },
   "source": [
    "### Complete the following code to collect results from multiple cities and starting points. \n",
    "- Indeed.com only has salary information for an estimated 20% of job postings. You may want to add other cities to the list below to gather more data. \n",
    "- Remember to convert your salary to U.S. Dollars to match the other cities if the currency is different"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-17T00:33:31.710943",
     "start_time": "2016-10-17T00:05:15.512276"
    },
    "collapsed": false,
    "focus": false,
    "id": "04b0f9af-540e-402f-8292-81748707c676"
   },
   "outputs": [
    {
     "ename": "WebDriverException",
     "evalue": "Message: 'phantomJS' executable needs to be in PATH. \n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mWebDriverException\u001b[0m                        Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-b14d70944523>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     89\u001b[0m \u001b[0mdcap\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"phantomjs.page.settings.userAgent\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/53.0.2785.143 Safari/537.36\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m \u001b[0mdriver\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwebdriver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPhantomJS\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mexecutable_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'/Applications/anaconda/anaconda/bin/phantomJS'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesired_capabilities\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdcap\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m \u001b[0mdriver\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_window_size\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1024\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m768\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Jesse/anaconda/lib/python2.7/site-packages/selenium/webdriver/phantomjs/webdriver.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, executable_path, port, desired_capabilities, service_args, service_log_path)\u001b[0m\n\u001b[1;32m     50\u001b[0m             \u001b[0mservice_args\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mservice_args\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             log_path=service_log_path)\n\u001b[0;32m---> 52\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mservice\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Jesse/anaconda/lib/python2.7/site-packages/selenium/webdriver/common/service.pyc\u001b[0m in \u001b[0;36mstart\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     69\u001b[0m                 raise WebDriverException(\n\u001b[1;32m     70\u001b[0m                     \"'%s' executable needs to be in PATH. %s\" % (\n\u001b[0;32m---> 71\u001b[0;31m                         os.path.basename(self.path), self.start_error_message)\n\u001b[0m\u001b[1;32m     72\u001b[0m                 )\n\u001b[1;32m     73\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0merr\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merrno\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0merrno\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mEACCES\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mWebDriverException\u001b[0m: Message: 'phantomJS' executable needs to be in PATH. \n"
     ]
    }
   ],
   "source": [
    "################################################################\n",
    "############### EDIT THESE CONSTANTS\n",
    "############### \n",
    "###############        EDIT THESE CONSTANTS\n",
    "\n",
    "MAX_RESULTS_PER_CITY = 1000      ### DO NOT SET MORE THAN 1000\n",
    "URL_SEARCH_TERM = 'Data Scientist' ### DO NOT SET MORE THAN SINGLE SEARCH TERM (TITLE)\n",
    "CITY_SET = ['New York', 'Chicago', 'San Francisco', 'Austin', 'Atlanta', '', 'Boston', 'Seattle'\\\n",
    "            'Los Angeles','Washington, DC', 'San Jose','Denver', 'Atlanta','Houston',\\\n",
    "            'Dallas','Nashville','San Diego','Cleveland','Minneapolis','Baltimore','Philadelphia','Detroit']\n",
    "###############\n",
    "################################################################\n",
    "\n",
    "\n",
    "import requests\n",
    "import bs4\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium.webdriver.common.desired_capabilities import DesiredCapabilities\n",
    "from selenium import webdriver\n",
    "import datetime\n",
    "import urllib\n",
    "\n",
    "def extract_location_from_resultRow(result):\n",
    "    try:\n",
    "        location = (result.find(class_='location').text.strip())\n",
    "    except:\n",
    "        location = ''\n",
    "    return location\n",
    "\n",
    "def extract_company_from_resultRow(result):\n",
    "    try:\n",
    "        company = (result.find(class_='company').text.strip())\n",
    "    except:\n",
    "        company = ''\n",
    "    return company\n",
    "\n",
    "def extract_jkid_from_resultRow(result):\n",
    "    try:\n",
    "        row = (result.find(class_='jobtitle turnstileLink'))\n",
    "        jkid = result['data-jk']\n",
    "    except: \n",
    "        jkid = ''\n",
    "    return jkid\n",
    "\n",
    "def extract_title_from_resultRow(result):\n",
    "    try:\n",
    "        title = (result.find(class_='turnstileLink'))\n",
    "        title_text = title.text\n",
    "    except: \n",
    "        title_text = ''\n",
    "    return title_text\n",
    "\n",
    "def extract_salary_from_resultRow(result):\n",
    "    try:\n",
    "        salary = (result.find(class_='snip').find('nobr').text)\n",
    "    except:\n",
    "        salary = ''\n",
    "    salary_text = salary\n",
    "    return salary_text\n",
    "\n",
    "def extract_reviews_from_resultRow(result):\n",
    "    try:\n",
    "        reviews = (result.find(class_='slNoUnderline').text.strip().strip(' reviews').replace(',',''))\n",
    "    except: \n",
    "        reviews = ''\n",
    "    return reviews\n",
    "\n",
    "def extract_stars_from_resultRow(result):\n",
    "    try: \n",
    "        stars = (result.find(class_='rating')['style']).split(';background-position:')[1].split(':')[1].split('px')[0].strip()\n",
    "    except: \n",
    "        stars = ''\n",
    "    return stars\n",
    "\n",
    "def extract_date_from_resultRow(result):\n",
    "    try: \n",
    "        date = (result.find(class_='date').text.strip(' ago').strip())\n",
    "    except: \n",
    "        date = ''\n",
    "    return date\n",
    "\n",
    "def extract_summary_from_resultRow(result):\n",
    "    try: \n",
    "        summary = (result.find(\"span\", {\"itemprop\" : \"description\"}).text.strip())\n",
    "    except: \n",
    "        summary = ''\n",
    "    return summary\n",
    "\n",
    "dcap = dict(DesiredCapabilities.PHANTOMJS)\n",
    "dcap[\"phantomjs.page.settings.userAgent\"] = (\"Mozilla/5.0 (Macintosh; Intel Mac OS X 10_12_0) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/53.0.2785.143 Safari/537.36\")\n",
    "\n",
    "driver = webdriver.PhantomJS(executable_path='/Users/Jesse/anaconda/bin/phantomJS', desired_capabilities=dcap)\n",
    "driver.set_window_size(1024, 768) \n",
    "\n",
    "for city in CITY_SET:\n",
    "    job_dict = []\n",
    "    now = datetime.datetime.now()\n",
    "    for start in range(0, MAX_RESULTS_PER_CITY, 10):\n",
    "\n",
    "        URL = \"http://www.indeed.com/jobs?q=\"+urllib.quote(URL_SEARCH_TERM)+\"&l=\"+urllib.quote(city)+\"&start=\"+str(start)\n",
    "        driver.get(URL)\n",
    "        soup = BeautifulSoup(driver.page_source, \"lxml\")\n",
    "\n",
    "        for i in soup.findAll(\"div\", {\"data-tn-component\" : \"organicJob\"}):\n",
    "\n",
    "            location = extract_location_from_resultRow(i)\n",
    "            company = extract_company_from_resultRow(i)\n",
    "            summary = extract_summary_from_resultRow(i)\n",
    "            jkid = extract_jkid_from_resultRow(i)\n",
    "            title = extract_title_from_resultRow(i)\n",
    "            salary = extract_salary_from_resultRow(i)\n",
    "            reviews = extract_reviews_from_resultRow(i)\n",
    "            stars = extract_stars_from_resultRow(i)\n",
    "            post_date = extract_date_from_resultRow(i)\n",
    "\n",
    "            job_dict.append([location, company, summary, jkid, title, salary, stars, reviews, post_date, now])\n",
    "            \n",
    "        job_df = pd.DataFrame(job_dict, columns=['location', 'company', 'summary', 'jkid', 'title', 'salary', 'stars', 'reviews', 'post_date', 'pull_date'])       \n",
    "\n",
    "    job_df.to_csv('scrape'+city+'_'+str(MAX_RESULTS_PER_CITY)+'.csv', encoding='utf-8')\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "focus": false,
    "id": "20339c09-5032-4e27-91be-286e9b46cd13"
   },
   "source": [
    "#### Use the functions you wrote above to parse out the 4 fields - location, title, company and salary. Create a dataframe from the results with those 4 columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "import datetime\n",
    "import urllib\n",
    "import urllib2\n",
    "import re\n",
    "from time import sleep # To prevent overwhelming the server between connections\n",
    "from collections import Counter # Keep track of our term counts\n",
    "from nltk.corpus import stopwords # Filter out stopwords, such as 'the', 'or', 'and'\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from bs4 import BeautifulSoup\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import auc\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "plt.style.use('ggplot')\n",
    "\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "from sklearn.cross_validation import train_test_split, cross_val_score\n",
    "from sklearn.linear_model import LogisticRegression, LogisticRegressionCV\n",
    "from sklearn.grid_search import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-17T09:00:32.995644",
     "start_time": "2016-10-17T09:00:32.867074"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/Jesse/anaconda/lib/python2.7/site-packages/pandas/core/ops.py:716: FutureWarning: elementwise comparison failed; returning scalar instead, but in the future will perform elementwise comparison\n",
      "  result = getattr(x, name)(y)\n"
     ]
    }
   ],
   "source": [
    "#load in the master csv file\n",
    "master_df = pd.read_csv('master.csv')\n",
    "\n",
    "# DELETE ANY HEADER ROWS LEFT OVER FROM CSV MERGE\n",
    "try: master_df = master_df[master_df['reviews'] != 'reviews'] \n",
    "except: pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Deleting unnamed column\n",
    "del master_df['Unnamed: 0']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-17T09:00:33.324025",
     "start_time": "2016-10-17T09:00:33.040902"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "###### REVIEWS CLEAN TO FLOAT\n",
    "######\n",
    "\n",
    "master_df['reviews'] = master_df['reviews'].fillna(0)\n",
    "\n",
    "def indeed_review_cleanup(review): \n",
    "    try:\n",
    "        review = review.str.replace(',','')\n",
    "        review = review.strip(' reviews')\n",
    "        review = review.strip(' review')\n",
    "        review = review.strip('reviews')\n",
    "        review = review.strip()\n",
    "        review = float(review)\n",
    "    except:\n",
    "        #print review\n",
    "        pass\n",
    "    return review\n",
    "\n",
    "master_df['clean_review'] = master_df[['reviews']].applymap(lambda x:indeed_review_cleanup(x))\n",
    "\n",
    "master_df['clean_review'].sort_values().unique()\n",
    "\n",
    "master_df['clean_review'] = master_df['clean_review'].astype(float)\n",
    "master_df['reviews'] = master_df['clean_review']\n",
    "master_df.drop('clean_review', axis=1, inplace=True)\n",
    "\n",
    "#########  END CLEAN REVIEWS\n",
    "###################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-17T09:00:33.548237",
     "start_time": "2016-10-17T09:00:33.326850"
    },
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "###### POST_DATE CLEAN TO FLOAT\n",
    "######\n",
    "\n",
    "try:\n",
    "    master_df['clean_post_date'] = master_df['post_date']\n",
    "except: pass\n",
    "\n",
    "\n",
    "def post_date_to_day_float(dateValue):\n",
    "    try:\n",
    "        temp = dateValue\n",
    "        dateValue.replace('s','')\n",
    "        if 'day' in dateValue:\n",
    "            temp = dateValue.split()[0]\n",
    "        elif 'hour' in dateValue:\n",
    "            temp = dateValue.split()[0]\n",
    "            temp = float(temp)/24\n",
    "        elif 'minute' in dateValue:\n",
    "            temp = dateValue.split()[0]\n",
    "            temp = float(temp)/24/60\n",
    "        if '+' in dateValue:\n",
    "            temp = 45           \n",
    "    except: \n",
    "        pass\n",
    "    return temp\n",
    "\n",
    "master_df['clean_post_date'] = master_df[['clean_post_date']].applymap(lambda x: post_date_to_day_float(x))\n",
    "\n",
    "master_df['clean_post_date'].sort_values().unique()\n",
    "\n",
    "master_df['clean_post_date'] = master_df['clean_post_date'].astype(float)\n",
    "master_df['post_date'] = master_df['clean_post_date']\n",
    "master_df.drop('clean_post_date', axis=1, inplace=True)\n",
    "master_df.rename(columns = {'post_date':'post_date_daysAgo'}, inplace=True)\n",
    "\n",
    "#########  END CLEAN POST_DATE\n",
    "###################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-17T09:00:33.630586",
     "start_time": "2016-10-17T09:00:33.551409"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "###### STARS CLEAN TO FLOAT\n",
    "######\n",
    "\n",
    "\n",
    "master_df['clean_stars'] = master_df['stars'].fillna(0)\n",
    "master_df['clean_stars'] = master_df[['stars']].astype(float).applymap(lambda x: x//6/2)\n",
    "\n",
    "\n",
    "master_df['stars'] = master_df['clean_stars']\n",
    "master_df.drop('clean_stars', axis=1, inplace=True)\n",
    "\n",
    "\n",
    "#########  END CLEAN STARS\n",
    "###################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-17T09:00:33.678361",
     "start_time": "2016-10-17T09:00:33.638226"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#####Create JOB_LINK column from JKID\n",
    "#####\n",
    "\n",
    "master_df['job_link'] = master_df[['jkid']].applymap(lambda x: 'http://www.indeed.com/rc/clk?jk='+x)\n",
    "\n",
    "#########  END JOB_LINK COLUMN\n",
    "###################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-17T09:00:34.448718",
     "start_time": "2016-10-17T09:00:34.210685"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##### Location Cleanup\n",
    "#####\n",
    "\n",
    "def location_cleanup(location):\n",
    "    temp = location\n",
    "    temp_city = location.split(',')[0]\n",
    "    try:\n",
    "        temp_state = location.split(',')[1].split()[0]\n",
    "    except: \n",
    "        temp_state = ''\n",
    "    return temp_city+\", \"+temp_state\n",
    "    \n",
    "master_df['location_clean'] = master_df[['location']].applymap(lambda x: location_cleanup(x))\n",
    "master_df['location_clean'].sort_values().unique()\n",
    "\n",
    "master_df['location'] = master_df['location_clean']\n",
    "master_df.drop('location_clean', axis=1, inplace=True)\n",
    "\n",
    "#########  END LOCATION CLEANUP COLUMN\n",
    "###################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-17T09:01:07.599900",
     "start_time": "2016-10-17T09:01:07.380588"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "##### Salary Cleanup\n",
    "#####\n",
    "\n",
    "master_df['salary'] = master_df['salary'].fillna(0)\n",
    "\n",
    "def cleanup_salary(salary):\n",
    "    if \"year\" in str(salary):\n",
    "        temp = salary.strip(\" a year\")\n",
    "        temp = temp.split('-')\n",
    "        low_range = int(temp[0].strip().replace(\"$\",\"\").replace(\",\",\"\"))\n",
    "        high_range = int(temp[-1].strip().replace(\"$\",\"\").replace(\",\",\"\"))\n",
    "        avg = (low_range + high_range) / 2\n",
    "        salary_list = [low_range,high_range,avg]\n",
    "    elif \"month\" in str(salary):\n",
    "        temp = salary.replace(\"a month\",\"\")\n",
    "        temp = temp.split('-')\n",
    "        low_range = int(temp[0].replace(\"$\",\"\").replace(\",\",\"\"))*12\n",
    "        high_range = int(temp[-1].replace(\"$\",\"\").replace(\",\",\"\"))*12\n",
    "        avg = (low_range + high_range) / 2\n",
    "        salary_list = [low_range,high_range,avg]\n",
    "    elif \"hour\" in str(salary):\n",
    "        temp = salary.replace(\"an hour\",\"\")\n",
    "        temp = temp.split('-')\n",
    "        low_range = float(temp[0].replace(\"$\",\"\").replace(\",\",\"\"))*2080\n",
    "        high_range = float(temp[-1].replace(\"$\",\"\").replace(\",\",\"\"))*2080\n",
    "        avg = (low_range + high_range) / 2\n",
    "        salary_list = [low_range,high_range,avg]\n",
    "    else:\n",
    "        salary_list = [0,0,0]\n",
    "        low_range = 0\n",
    "        high_range = 0\n",
    "        avg = 0\n",
    "        \n",
    "    return low_range, high_range, avg\n",
    "master_df['salary_clean'] = master_df[['salary']].applymap(lambda x: cleanup_salary(x))\n",
    "\n",
    "master_df['salary'] = master_df['salary_clean']\n",
    "\n",
    "master_df['sal_low'] = master_df['salary'].apply(lambda x: x[0])\n",
    "master_df['sal_high'] = master_df['salary'].apply(lambda x: x[1])\n",
    "master_df['sal_avg'] = master_df['salary'].apply(lambda x: x[2])\n",
    "\n",
    "master_df.drop('salary_clean', axis=1, inplace=True)\n",
    "master_df.drop('salary', axis=1, inplace=True)\n",
    "\n",
    "\n",
    "\n",
    "#########  END SALARY CLEANUP COLUMN\n",
    "###################\n",
    "#Add Comment\n",
    "# master_df['salary_clean'] = master_df[['salary']].applymap(lambda x: cleanup_salary(x))\n",
    "\n",
    "# master_df['salary'] = master_df['salary_clean']\n",
    "# master_df.drop('salary_clean', axis=1, inplace=True)\n",
    "\n",
    "#########  END SALARY CLEANUP COLUMN\n",
    "###################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2016-10-17T08:54:44.800034",
     "start_time": "2016-10-17T08:54:44.471688"
    },
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# has_salary = master_df[master_df['salary'] != (0,0,0)].shape[0]\n",
    "# all_records = master_df.shape[0]\n",
    "# print \"Job listings with salary info:\", has_salary\n",
    "# print \"Total job listings: \", all_records\n",
    "# print \"Salaried listings / Total listings\", round((float(has_salary) / all_records) * 100, 3), '%'\n",
    "# master_df.head(5)\n",
    "# master_df['title'].sort_values().unique()\n",
    "\n",
    "# stacked = pd.DataFrame(master_df['summary'].str.split().tolist()).stack()\n",
    "# final = pd.DataFrame(stacked.value_counts())\n",
    "# final.reset_index(inplace=True)\n",
    "# final['unique'] = final['index'].sort_values().unique()\n",
    "# final['unique']\n",
    "# import nltk\n",
    "# final['tagged'] = final[['index']].applymap(lambda x: nltk.pos_tag(x.strip()))\n",
    "# final.info()\n",
    "# master_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#changing title, search_city, and company to lower case\n",
    "master_df['title'] = master_df['title'].apply(lambda x: str(x).lower())\n",
    "master_df['search_city'] = master_df['search_city'].apply(lambda x: str(x).lower())\n",
    "master_df['company'] = master_df['company'].apply(lambda x: str(x).lower())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Bin the indeed df (master_df) into entry/mid/senior bins\n",
    "senior = ['sr','senior','lead','instructor','principal', 'director','manager','consultant','chief']\n",
    "mid = ['data','scientist','analyst','analytics','statistician',\"statistical\",'machine learning']\n",
    "entry = ['associate','Associate','intern','junior','-1']\n",
    "senior_bin = []\n",
    "mid_bin = []\n",
    "entry_bin = []\n",
    "other_bin = []\n",
    "for x in master_df['title']:\n",
    "    if any(word in x.lower() for word in senior):\n",
    "        senior_bin.append(1)\n",
    "        mid_bin.append(0)\n",
    "        entry_bin.append(0)\n",
    "        other_bin.append(0)\n",
    "    elif any(word in x.lower() for word in entry):\n",
    "        senior_bin.append(0)\n",
    "        mid_bin.append(0)\n",
    "        entry_bin.append(1)\n",
    "        other_bin.append(0)        \n",
    "    elif any(word in x.lower() for word in mid):\n",
    "        senior_bin.append(0)\n",
    "        mid_bin.append(1)\n",
    "        entry_bin.append(0)\n",
    "        other_bin.append(0)        \n",
    "    else:\n",
    "        senior_bin.append(0)\n",
    "        mid_bin.append(0)\n",
    "        entry_bin.append(0)\n",
    "        other_bin.append(1)        \n",
    "master_df['senior_bin'] = pd.Series(senior_bin)\n",
    "master_df['mid_bin'] = pd.Series(mid_bin)\n",
    "master_df['entry_bin'] = pd.Series(entry_bin)\n",
    "master_df['other_bin'] = pd.Series(other_bin)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scraping from www.expatistan.com to get Cost of Living index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#scraping to get cost of living index\n",
    "URL = 'https://www.expatistan.com/cost-of-living/country/united-states' \n",
    "driver = webdriver.PhantomJS(executable_path='/Users/Jesse/anaconda/bin/phantomJS')\n",
    "driver.set_window_size(1024, 768) \n",
    "driver.get(URL)\n",
    "soup = BeautifulSoup(driver.page_source,'lxml')\n",
    "#print soup.prettify()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#scraping to get cost of living index - converting to dataframe\n",
    "c = []\n",
    "r = []\n",
    "for city in soup.findAll('table',class_=\"city-index extra-spacing-in-mobile\"):\n",
    "    for cities in soup.findAll('td',class_='city-name'):\n",
    "        c.append(cities.text)\n",
    "    for rank in soup.findAll('td',class_='price-index'):\n",
    "        r.append(rank.text)\n",
    "\n",
    "#forming the data frame\n",
    "COL = pd.DataFrame([c, r]).T\n",
    "#doing the cleaning to get ready for merge with glassdoor\n",
    "COL = COL.rename(columns = {0:'Cities',1:'price_index'})\n",
    "COL['Cities'] = COL['Cities'].apply(lambda x: str(x).split(',')).apply(lambda x: x[0]).apply(lambda x: str(x).replace(' (United States)',''))\n",
    "COL['price_index'] = COL['price_index'].astype(int)\n",
    "#creating a new column to have the ratio as NYC as the base\n",
    "new_base  = COL.loc[1,'price_index']\n",
    "COL['COL_new'] = COL['price_index'].apply(lambda x: float(new_base)/x)\n",
    "#clean the city\n",
    "COL.loc[21,'Cities'] = 'Minneapolis'\n",
    "COL.loc[4,'Cities'] = 'Washington DC'\n",
    "COL.loc[1,'Cities'] = 'New York'\n",
    "COL.ix[70,'Cities'] = 'Cincinatti'\n",
    "#converting cities column to lower\n",
    "COL['Cities'] = COL['Cities'].apply(lambda x: str(x).lower())\n",
    "\n",
    "master_df = pd.merge(master_df,COL.iloc[:,[0,-1]], left_on='search_city',right_on='Cities',how='left')\n",
    "del master_df['Cities']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#limited the master_df (indeed) to only consider data scientist and data to be used as a test for our predictor\n",
    "master_df = master_df.ix[master_df['title'].str.contains('data scientist','data'),:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Glassdoor scraping data\n",
    "\n",
    "The scraping code was forked from https://github.com/ashalan/glassdoor-salary-scraper. This will allow us to scrape glassdoor which outputs to JSON files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Expected object or value",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-23-e69a501e4444>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mcity\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mcities\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0;34m\"./glassdoor-salary-scraper-master/\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mcity\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\".json\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m     \u001b[0mcity_sub\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0mcity_sub\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'City'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcity\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0mglassdoor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconcat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mglassdoor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcity_sub\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Jesse/anaconda/lib/python2.7/site-packages/pandas/io/json.pyc\u001b[0m in \u001b[0;36mread_json\u001b[0;34m(path_or_buf, orient, typ, dtype, convert_axes, convert_dates, keep_default_dates, numpy, precise_float, date_unit)\u001b[0m\n\u001b[1;32m    209\u001b[0m         obj = FrameParser(json, orient, dtype, convert_axes, convert_dates,\n\u001b[1;32m    210\u001b[0m                           \u001b[0mkeep_default_dates\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprecise_float\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 211\u001b[0;31m                           date_unit).parse()\n\u001b[0m\u001b[1;32m    212\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    213\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtyp\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'series'\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Jesse/anaconda/lib/python2.7/site-packages/pandas/io/json.pyc\u001b[0m in \u001b[0;36mparse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    278\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 279\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parse_no_numpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    280\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    281\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Jesse/anaconda/lib/python2.7/site-packages/pandas/io/json.pyc\u001b[0m in \u001b[0;36m_parse_no_numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    494\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0morient\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"columns\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m             self.obj = DataFrame(\n\u001b[0;32m--> 496\u001b[0;31m                 loads(json, precise_float=self.precise_float), dtype=None)\n\u001b[0m\u001b[1;32m    497\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0morient\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"split\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    498\u001b[0m             decoded = dict((str(k), v)\n",
      "\u001b[0;31mValueError\u001b[0m: Expected object or value"
     ]
    }
   ],
   "source": [
    "#bringing in glassdoor data by a for loop and creating a dataframe\n",
    "cities = ['atlanta','austin','boston','dallas','detroit','houston','kansas-city-mo','los-angeles',\\\n",
    "         'minneapolis','nashville','new-york-city','san-francisco','san-jose','seattle','washington-dc']\n",
    "glassdoor = pd.DataFrame(columns = ['company','jobTitle','meanPay','City'])\n",
    "for city in cities:\n",
    "    path= \"./glassdoor-salary-scraper-master/\"+city+\".json\"\n",
    "    city_sub = pd.read_json(path)\n",
    "    city_sub['City'] = city\n",
    "    glassdoor = pd.concat([glassdoor,city_sub])\n",
    "glassdoor.reset_index(inplace = True,drop=True)\n",
    "\n",
    "#cleaning the columns for City\n",
    "glassdoor['City'] = glassdoor['City'].apply(lambda x: str(x).replace('-',' '))\n",
    "\n",
    "#cleaning and converting into string into a column for low sal, high sal, and average sal\n",
    "glassdoor['meanPay'] = glassdoor['meanPay'].apply(lambda x: str(x).replace('k','000')).apply(lambda x: str(x).replace('$',''))\n",
    "glassdoor['meanPay'] = glassdoor['meanPay'].apply(lambda x: str(x).replace(',',''))\n",
    "glassdoor['low_sal'] = glassdoor['meanPay'].apply(lambda x: str(x).split(' - ')).apply(lambda x: x[0]).astype(float)\n",
    "glassdoor['high_sal'] = glassdoor['meanPay'].apply(lambda x: str(x).split(' - ')).apply(lambda x: x[-1]).astype(float)\n",
    "\n",
    "#converting the hourly wages\n",
    "\n",
    "hourly_list = glassdoor[glassdoor['low_sal'] < 1000].index.tolist()\n",
    "hourly_list\n",
    "for x in hourly_list:\n",
    "    glassdoor.loc[x,'low_sal'] = glassdoor.get_value(x,'low_sal')*2080\n",
    "    glassdoor.loc[x,'high_sal'] = glassdoor.get_value(x,'high_sal')*2080\n",
    "#converting the monthly wages\n",
    "\n",
    "    monthly_list = glassdoor[glassdoor['low_sal'] < 10000].index.tolist()\n",
    "for x in monthly_list:\n",
    "    glassdoor.loc[x,'low_sal'] = glassdoor.get_value(x,'low_sal')*12\n",
    "    glassdoor.loc[x,'high_sal'] = glassdoor.get_value(x,'high_sal')*12\n",
    "\n",
    "#converting salary into integers    \n",
    "glassdoor['low_sal'] = glassdoor['low_sal'].astype(int)\n",
    "glassdoor['high_sal'] = glassdoor['high_sal'].astype(int)\n",
    "\n",
    "#finding median salary\n",
    "\n",
    "sal_avg = []\n",
    "for x,y in zip(glassdoor['low_sal'],glassdoor['high_sal']):\n",
    "    if y > x:\n",
    "        sal_avg.append(x + (y-x)/2)\n",
    "    else:\n",
    "        sal_avg.append(y)\n",
    "glassdoor['sal_avg'] = sal_avg\n",
    "\n",
    "#glassdoor['company'] = glassdoor['company'].apply(lambda x: str(x).upper())\n",
    "\n",
    "#changing company from upper to lower case\n",
    "\n",
    "company = []\n",
    "for x in glassdoor['company']:\n",
    "    if type(x) == 'unicode':\n",
    "        company.append(x)\n",
    "    else:\n",
    "        company.append(x.encode('ascii','replace'))\n",
    "company_lower = []\n",
    "for x in company:\n",
    "    company_lower.append(x.lower())\n",
    "glassdoor['company'] = company_lower\n",
    "glassdoor = glassdoor.rename(columns = {'jobTitle':'title'})\n",
    "\n",
    "#creating title bins for senior, mid, and entry level\n",
    "\n",
    "senior = ['sr','senior','lead','instructor','principal', 'director','manager','consultant','chief']\n",
    "mid = []\n",
    "entry = ['associate','Associate','intern','junior','-1','entry_level',' I']\n",
    "senior_bin = []\n",
    "mid_bin = []\n",
    "entry_bin = []\n",
    "for x in glassdoor['title']:\n",
    "    if any(word in x.lower() for word in senior):\n",
    "        senior_bin.append(1)\n",
    "        mid_bin.append(0)\n",
    "        entry_bin.append(0)\n",
    "    elif any(word in x.lower() for word in entry):\n",
    "        senior_bin.append(0)\n",
    "        mid_bin.append(0)\n",
    "        entry_bin.append(1)\n",
    "    else:\n",
    "        senior_bin.append(0)\n",
    "        mid_bin.append(1)\n",
    "        entry_bin.append(0)\n",
    "        \n",
    "glassdoor['entry_bin'] = entry_bin\n",
    "glassdoor['mid_bin'] = mid_bin\n",
    "glassdoor['senior_bin'] = senior_bin\n",
    "\n",
    "#converting title and city to lower case\n",
    "glassdoor['title'] = glassdoor['title'].apply(lambda x: str(x).lower())\n",
    "glassdoor['City'] = glassdoor['City'].apply(lambda x: x.replace('kansas city mo','kansas city'))\n",
    "\n",
    "#exporting cleaned dataframe\n",
    "glassdoor.to_csv('glassdoor_df.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'sal_avg'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-189c2078f49f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mdel\u001b[0m \u001b[0mgd\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Cities'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;32mdel\u001b[0m \u001b[0mgd\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'price_index'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m \u001b[0mmedian_sal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgd\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sal_avg'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmedian\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m \u001b[0mmedian_sal_sr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgd\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mgd\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'senior_bin'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sal_avg'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmedian\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0mmedian_sal_mid\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgd\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mgd\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'mid_bin'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'sal_avg'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmedian\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Jesse/anaconda/lib/python2.7/site-packages/pandas/core/frame.pyc\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1995\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_multilevel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1996\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1997\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_getitem_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1998\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1999\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_getitem_column\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Jesse/anaconda/lib/python2.7/site-packages/pandas/core/frame.pyc\u001b[0m in \u001b[0;36m_getitem_column\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   2002\u001b[0m         \u001b[0;31m# get column\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2003\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_unique\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2004\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_item_cache\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2005\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2006\u001b[0m         \u001b[0;31m# duplicate columns & possible reduce dimensionality\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Jesse/anaconda/lib/python2.7/site-packages/pandas/core/generic.pyc\u001b[0m in \u001b[0;36m_get_item_cache\u001b[0;34m(self, item)\u001b[0m\n\u001b[1;32m   1348\u001b[0m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1349\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mres\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1350\u001b[0;31m             \u001b[0mvalues\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_data\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1351\u001b[0m             \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_box_item_values\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalues\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1352\u001b[0m             \u001b[0mcache\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mres\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Jesse/anaconda/lib/python2.7/site-packages/pandas/core/internals.pyc\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, item, fastpath)\u001b[0m\n\u001b[1;32m   3288\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3289\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misnull\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3290\u001b[0;31m                 \u001b[0mloc\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mitem\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3291\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3292\u001b[0m                 \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0misnull\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/Jesse/anaconda/lib/python2.7/site-packages/pandas/indexes/base.pyc\u001b[0m in \u001b[0;36mget_loc\u001b[0;34m(self, key, method, tolerance)\u001b[0m\n\u001b[1;32m   1945\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1946\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1947\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_loc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_cast_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1948\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1949\u001b[0m         \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmethod\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmethod\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtolerance\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtolerance\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mpandas/index.pyx\u001b[0m in \u001b[0;36mpandas.index.IndexEngine.get_loc (pandas/index.c:4154)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/index.pyx\u001b[0m in \u001b[0;36mpandas.index.IndexEngine.get_loc (pandas/index.c:4018)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/hashtable.pyx\u001b[0m in \u001b[0;36mpandas.hashtable.PyObjectHashTable.get_item (pandas/hashtable.c:12368)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32mpandas/hashtable.pyx\u001b[0m in \u001b[0;36mpandas.hashtable.PyObjectHashTable.get_item (pandas/hashtable.c:12322)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'sal_avg'"
     ]
    }
   ],
   "source": [
    "#merging glassdoor with cost of living index dataframe\n",
    "\n",
    "gd = pd.merge(glassdoor, COL,left_on = \"City\",right_on = \"Cities\")\n",
    "\n",
    "#calculates the mean salary or we can use median\n",
    "del gd['Cities']\n",
    "del gd['price_index']\n",
    "median_sal = gd['sal_avg'].median()\n",
    "median_sal_sr = gd[gd['senior_bin']==1]['sal_avg'].median()\n",
    "median_sal_mid = gd[gd['mid_bin']==1]['sal_avg'].median()\n",
    "median_sal_entry = gd[gd['entry_bin']==1]['sal_avg'].median()\n",
    "\n",
    "#finding the median salary on glassdoor dataframe\n",
    "\n",
    "print 'median salary is:',median_sal\n",
    "\n",
    "# print median_sal_sr\n",
    "# print median_sal_mid\n",
    "# print median_sal_entry\n",
    "\n",
    "#creating a normalized salaries which is using NYC as a base\n",
    "\n",
    "gd['norm_sal'] = gd['sal_avg'] * gd['COL_new']\n",
    "median_norm_sal = gd['norm_sal'].median()\n",
    "gd['above_median'] = gd['norm_sal'].apply(lambda x: 1 if x > median_norm_sal else 0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Histograms\n",
    "\n",
    "Plotting hisograms of the median salaries. The histograms show that we are working with skewed data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "print 'Count of Senior-level bin:',gd[gd['senior_bin']==1]['sal_avg'].count()\n",
    "print 'Count of Mid-level bin:',gd[gd['mid_bin']==1]['sal_avg'].count()\n",
    "print 'Count of Entry-level bin:',gd[gd['entry_bin']==1]['sal_avg'].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2,2, figsize=(20,10))\n",
    "#Bottles Sold less than or equal to 10\n",
    "axes[0][0].hist(x = gd['sal_avg'],bins = 20,color = 'crimson') \n",
    "axes[0][0].set_title('Distribution of Average Salaries',fontsize = 22)\n",
    "#axes[0][0].xaxis.set_ticks(np.arange(0, 21, 2))\n",
    "#axes[0][0].yaxis.set_ticks(np.arange(0, 70000, 5000))\n",
    "axes[0][1].hist(x = gd[gd['entry_bin']==1]['sal_avg'],bins = 20,color = 'blue') \n",
    "axes[0][1].set_title('Distribution of Entry-Level Average Salaries',fontsize = 22)\n",
    "axes[1][0].hist(x = gd[gd['mid_bin']==1]['sal_avg'],bins = 20,color = 'darkorange' )\n",
    "axes[1][0].set_title('Distribution of Mid-Level Average Salaries',fontsize = 22)\n",
    "axes[1][1].hist(x = gd[gd['senior_bin']==1]['sal_avg'],bins = 20,color = 'green' )\n",
    "axes[1][1].set_title('Distribution of Senior-Level Average Salaries',fontsize = 22);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The histograms above are very skewed. This because there is bias due to cost of living. New York and Bay area are expensives cities to live in, thus needs to be normalized via with cost of living\n",
    "\n",
    "#### Histograms using normalized salaries (with Cost of Living)\n",
    "\n",
    "Normalizing the median salary by multiplying the ratio of new york COL divided by the city COL gets expected salary for New York. This allows all salaries to be compared as if it was based in New York. The data becomes more normalized than using salaries without cost of living. The entry-level histogram is skewed perhaps due to low sample size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(2,2, figsize=(20,10))\n",
    "#Bottles Sold less than or equal to 10\n",
    "axes[0][0].hist(x = gd['norm_sal'],bins = 20,color = 'crimson') \n",
    "axes[0][0].set_title('Distribution of Average Salaries',fontsize = 22)\n",
    "#axes[0][0].xaxis.set_ticks(np.arange(0, 21, 2))\n",
    "#axes[0][0].yaxis.set_ticks(np.arange(0, 70000, 5000))\n",
    "axes[0][1].hist(x = gd[gd['entry_bin']==1]['norm_sal'],bins = 20,color = 'blue') \n",
    "axes[0][1].set_title('Distribution of Entry-Level Average Salaries',fontsize = 22)\n",
    "axes[1][0].hist(x = gd[gd['mid_bin']==1]['norm_sal'],bins = 20,color = 'darkorange' )\n",
    "axes[1][0].set_title('Distribution of Mid-Level Average Salaries',fontsize = 22)\n",
    "axes[1][1].hist(x = gd[gd['senior_bin']==1]['norm_sal'],bins = 20,color = 'green' )\n",
    "axes[1][1].set_title('Distribution of Senior-Level Average Salaries',fontsize = 22);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assumptions\n",
    "\n",
    "Used Glassdoor to scrape salaries and expatistan.com to scrape cost of living\n",
    "\n",
    "1) normalizing salaries to adjust for Cost of Living, using NYC as base\n",
    "\n",
    "2) Used only experience, and cities as the features due not having access to skills in glassdoor\n",
    "\n",
    "3) There will be bias due to not having access to bonus or perks\n",
    "\n",
    "4) We could use industry to determine if there is a correlation with sector and salaries\n",
    "\n",
    "\n",
    "## Regression\n",
    "\n",
    "#### Target Variable: \n",
    "* Above_mean (1 if above mean or 0 below mean)\n",
    "\n",
    "#### Features:\n",
    "* Cities (Dummies)\n",
    "* Entry-level (Dummy)\n",
    "* Mid-level (Dummy)\n",
    "* Senior-Level (Dummy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "features = gd.loc[:,['City','entry_bin','mid_bin','senior_bin']]\n",
    "X = pd.get_dummies(features,columns=['City'])\n",
    "y = gd['above_median']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_train, X_test, Y_train, Y_test = train_test_split(X, y, test_size = 0.33, random_state = 77) ## create train-test out of the data given"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# this is the code from the lab, using gridsearchCV\n",
    "#need to expand on C_vals\n",
    "\n",
    "logreg = LogisticRegression(solver='liblinear')\n",
    "C_vals = [.0001,.001,.01,.05, .1,.12,.13,.135,.15,.16,.17,1,3,5,10,20,50,100]\n",
    "#C_vals = np.linspace(.33,.66,50)\n",
    "penalties = ['l1','l2']\n",
    "\n",
    "#using gridsearch to find best penalty and C\n",
    "\n",
    "gs = GridSearchCV(logreg, {'penalty': penalties, 'C': C_vals}, verbose=False, cv=3)\n",
    "gs.fit(X_train, Y_train)\n",
    "\n",
    "print gs.best_params_\n",
    "logreg = LogisticRegression(C=gs.best_params_['C'], penalty=gs.best_params_['penalty'])\n",
    "#logreg = LogisticRegression(C=.55, penalty=gs.best_params_['penalty'])\n",
    "cv_model = logreg.fit(X_train, Y_train)\n",
    "cv_pred = cv_model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cv_model.coef_\n",
    "#need to get coeficients into a list to bring in as a dataframe to pair up with columns\n",
    "\n",
    "\n",
    "#coef_list = cv_model.coef_.tolist("
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "y_score = cv_model.decision_function(X_test)\n",
    "conmat = np.array(confusion_matrix(Y_test, cv_pred, labels=[1,0]))\n",
    "confusion = pd.DataFrame(conmat,index=['over_mean', 'under_mean'],\n",
    "                         columns=['predicted_over_mean','predicted_under_mean'])\n",
    "print confusion\n",
    "\n",
    "print classification_report(Y_test,cv_pred)\n",
    "print \"AUC Score is:\",roc_auc_score(Y_test, y_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 133 / 188 = TP / (TP + FP)\n",
    "float(conmat[0,0]) / conmat[:,0].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "roc_auc_score(Y_test, y_score)\n",
    "\n",
    "\"\"\" Plot AUC\"\"\"\n",
    "\n",
    "FPR = dict()\n",
    "TPR = dict()\n",
    "ROC_AUC = dict()\n",
    "\n",
    "# For class 1, find the area under the curve\n",
    "FPR[1], TPR[1], _ = roc_curve(Y_test, y_score)\n",
    "ROC_AUC[1] = auc(FPR[1], TPR[1])\n",
    "\n",
    "# Plot of a ROC curve for class 1 (has_cancer)\n",
    "plt.figure(figsize=[11,9])\n",
    "plt.plot(FPR[1], TPR[1], label='ROC curve (area = %0.2f)' % ROC_AUC[1], linewidth=4,color='blue')\n",
    "plt.plot([0, 1], [0, 1], 'k--', linewidth=4)\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate', fontsize=18)\n",
    "plt.ylabel('True Positive Rate', fontsize=18)\n",
    "plt.title('Receiver operating characteristic for high/low income for Glassdoor', fontsize=18)\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For glassdoor, the gridsearch resulted in penalty being L2, and C:10\n",
    "\n",
    "#### Regression  #2  - dropping out Houston and Kansas City to test against Indeed dataframe\n",
    "\n",
    "Dropping Houston and Kansas City. Will re-run the initial model and then predicting on indeed dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#cities we want to include\n",
    "cities = ['atlanta','austin','boston','dallas','detroit','los angeles','minneapolis','nashville','san francisco','san jose','seattle','washington dc']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#dropping houston and kc from the original glassdoor dataframe so that we can predict on indeed\n",
    "gd_1 = gd[gd['City'].isin(cities)]\n",
    "features_1 = gd_1.loc[:,['City','entry_bin','mid_bin','senior_bin']]\n",
    "y_test = gd_1['above_median']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#setting the X to use to fit the new model (without Houston and Kansas City)\n",
    "X_1 = pd.get_dummies(features_1,columns = ['City'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#prepping the indeed dataframe to be used as a test\n",
    "cities = ['atlanta','austin','boston','dallas','detroit','houston','kanasas city','los angeles','minneapolis','nashville','san francisco','san jose','seattle','washington, dc']\n",
    "\n",
    "master_select_cities = master_df[master_df['search_city'].isin(cities)]\n",
    "master_feature = master_select_cities[master_select_cities['sal_avg'] > 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#setting the X data for indeed to be used in the test\n",
    "X_indeed = master_feature.loc[:,['search_city','entry_bin','mid_bin','senior_bin']]\n",
    "X_indeed = pd.get_dummies(X_indeed,columns=['search_city'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#creating the binary target variable (above_mean) for the indeed test\n",
    "master_feature\n",
    "master_feature['norm_sal'] = master_feature['sal_avg'] * master_feature['COL_new']\n",
    "master_feature['above_median'] = master_feature['norm_sal'].apply(lambda x: 1 if x > median_norm_sal else 0)\n",
    "y_indeed = master_feature['above_median']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#creating the train/test splits\n",
    "X_train1, X_test1, Y_train1, Y_test1 = train_test_split(X_1, y_test, test_size = 0.33, random_state = 77) ## create train-test out of the data given"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#performing the regression with houston and kc being dropped out\n",
    "logreg = LogisticRegression(solver='liblinear')\n",
    "C_vals = [.0001,.001,.01,.05, .1,.12,.13,.135,.15,.16,.17,1,3,5,10,20,50,100]\n",
    "#C_vals = np.linspace(.33,.66,50)\n",
    "penalties = ['l1','l2']\n",
    "\n",
    "#using gridsearch to find best penalty and C\n",
    "\n",
    "gs = GridSearchCV(logreg, {'penalty': penalties, 'C': C_vals}, verbose=False, cv=3)\n",
    "gs.fit(X_train1, Y_train1)\n",
    "\n",
    "print gs.best_params_\n",
    "logreg = LogisticRegression(C=gs.best_params_['C'], penalty=gs.best_params_['penalty'])\n",
    "#logreg = LogisticRegression(C=.55, penalty=gs.best_params_['penalty'])\n",
    "cv_model_1 = logreg.fit(X_train1, Y_train1)\n",
    "cv_pred_1 = cv_model_1.predict(X_test1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#finding the metrics of the new model\n",
    "y_score_1 = cv_model_1.decision_function(X_test1)\n",
    "conmat_1 = np.array(confusion_matrix(Y_test1, cv_pred_1, labels=[1,0]))\n",
    "confusion_1 = pd.DataFrame(conmat,index=['over_mean', 'under_mean'],\n",
    "                         columns=['predicted_over_mean','predicted_under_mean'])\n",
    "print confusion_1\n",
    "\n",
    "print classification_report(Y_test1,cv_pred_1)\n",
    "print \"AUC Score is:\",roc_auc_score(Y_test1, y_score_1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# 133 / 188 = TP / (TP + FP)\n",
    "float(conmat[0,0]) / conmat[:,0].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#plotting the AUC curve\n",
    "roc_auc_score(Y_test1, y_score_1)\n",
    "\n",
    "\"\"\" Plot AUC\"\"\"\n",
    "\n",
    "FPR = dict()\n",
    "TPR = dict()\n",
    "ROC_AUC = dict()\n",
    "\n",
    "# For class 1, find the area under the curve\n",
    "FPR[1], TPR[1], _ = roc_curve(Y_test1, y_score_1)\n",
    "ROC_AUC[1] = auc(FPR[1], TPR[1])\n",
    "\n",
    "# Plot of a ROC curve for class 1 (has_cancer)\n",
    "plt.figure(figsize=[11,9])\n",
    "plt.plot(FPR[1], TPR[1], label='ROC curve (area = %0.2f)' % ROC_AUC[1], linewidth=4,color='blue')\n",
    "plt.plot([0, 1], [0, 1], 'k--', linewidth=4)\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate', fontsize=18)\n",
    "plt.ylabel('True Positive Rate', fontsize=18)\n",
    "plt.title('Receiver operating characteristic for high/low income', fontsize=18)\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dropping out Houston and KC reduced the AUC score by 0.003, which is a very marginal loss.\n",
    "\n",
    "#### Using the model to predict on Indeed dataframe\n",
    "We do not have enough data to judge accuracy of the model. Indeed has an excessive amount of missing salary data, which makes this task difficult. That being said, the AUC is .84, which is 0.03 less than the AUC for glassdoor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cv_pred_indeed = cv_model_1.predict(X_indeed)\n",
    "y_score_indeed = cv_model_1.decision_function(X_indeed)\n",
    "conmat_indeed = np.array(confusion_matrix(y_indeed, cv_pred_indeed, labels=[1,0]))\n",
    "confusion_indeed = pd.DataFrame(conmat_indeed,index=['over_mean', 'under_mean'],\n",
    "                         columns=['predicted_over_mean','predicted_under_mean'])\n",
    "print confusion_indeed\n",
    "\n",
    "print classification_report(y_indeed,cv_pred_indeed)\n",
    "print \"AUC Score is:\",roc_auc_score(y_indeed, y_score_indeed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "roc_auc_score(y_indeed, y_score_indeed)\n",
    "\n",
    "\"\"\" Plot AUC\"\"\"\n",
    "\n",
    "FPR = dict()\n",
    "TPR = dict()\n",
    "ROC_AUC = dict()\n",
    "\n",
    "# For class 1, find the area under the curve\n",
    "FPR[1], TPR[1], _ = roc_curve(y_indeed, y_score_indeed)\n",
    "ROC_AUC[1] = auc(FPR[1], TPR[1])\n",
    "\n",
    "# Plot of a ROC curve for class 1 (has_cancer)\n",
    "plt.figure(figsize=[11,9])\n",
    "plt.plot(FPR[1], TPR[1], label='ROC curve (area = %0.2f)' % ROC_AUC[1], linewidth=4,color='blue')\n",
    "plt.plot([0, 1], [0, 1], 'k--', linewidth=4)\n",
    "plt.xlim([0.0, 1.0])\n",
    "plt.ylim([0.0, 1.05])\n",
    "plt.xlabel('False Positive Rate', fontsize=18)\n",
    "plt.ylabel('True Positive Rate', fontsize=18)\n",
    "plt.title('Receiver operating characteristic for high/low income on Indeed', fontsize=18)\n",
    "plt.legend(loc=\"lower right\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
